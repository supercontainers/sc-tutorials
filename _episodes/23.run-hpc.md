---
title: "Running HPC jobs with containers"
teaching: 40
exercises: 40
questions:
- How can I execute commands in a container with Singularity or Shifter?
- How are variables and directories shared between host and container?
- Is it possible to simplify the container user experience?
objectives:
- Download and run containers on a supercomputer
- Manage sharing of variables and directories with the host
- Run a real-world bioinformatics application in a container
- Use container modules to *hide* the container runtime syntax
keypoints:
- Download a container image with `singularity pull` or `shifterimg pull`
- Execute commands in a container with `singularity exec` or `shifter`
- By default Singularity and Shifter pass all host variables to the container
- By default Singularity and Shifter use the host current directory as the container working directory
- Define container specific shell variables with Singularity by prefixing them with `SINGULARITYENV_`
- Mount additional host directories with Singularity with the flag `-B`, or the variable `SINGULARITY_BINDPATH`
---


### Download and run containers

Before starting, let us cd into the `exercises` subdirectory of the tutorial repository directory (see also Setup page):

```bash
cd ~/sc-tutorials/exercises
```


#### Singularity

Download ECP's Ubuntu 22.04 image using:

```bash
singularity pull docker://ecpe4s/ubuntu22.04
```
```output
INFO:    Converting OCI blobs to SIF format
INFO:    Starting build...
Getting image source signatures
Copying blob d19f32bd9e41 done
Copying blob 63f70a975ff1 done
Copying blob f7c04a75ff42 done
...
Copying config 0cad836100 done
Writing manifest to image destination
Storing signatures
2023/10/27 06:50:17  info unpack layer: sha256:d19f32bd9e4106d487f1a703fc2f09c8edadd92db4405d477978e8e466ab290d
2023/10/27 06:50:18  info unpack layer: sha256:63f70a975ff19fb1c92fc6a8ef75b85644211c8ca37f56aecc99629b9444be3c
2023/10/27 06:50:19  info unpack layer: sha256:f7c04a75ff427331d2133d2bfebd3358a7e1a1d6448de27c0920f13f64c95af5
2023/10/27 06:50:19  warn rootless{usr/bin/ping} ignoring (usually) harmless EPERM on setxattr "security.capability"
...
2023/10/27 06:50:50  info unpack layer: sha256:d91732cb657c274bfef42cb46ddad946c6670575e33b37434742c5c32c00f4a6
INFO:    Creating SIF file...
```

Note how you need to prepend `docker://` to the image name, to tell Singularity you're downloading an image in Docker format (the default would be to download a SIF image).

The image file is just in your current directory:

```bash
ls
```
```output
ubuntu22.04_latest.sif
```

Now let's execute some Linux commands from within the container, `whoami` and `cat /etc/os-release`:

```bash
singularity exec ubuntu22.04_latest.sif whoami
```
```output
tutorial
```

```bash
singularity exec ubuntu22.04_latest.sif cat /etc/os-release
```
```output
PRETTY_NAME="Ubuntu 22.04.1 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.1 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
```

Note how with Singularity the user in the container is the same as in the host machine.

Singularity has a dedicated syntax to open an interactive shell prompt in a container:

```bash
singularity shell ubuntu22.04_latest.sif
```
```output
Singularity>
```

Exit the shell by typing `exit` or hitting `Ctrl-D`.

Finally, note you can request Singularity to execute a container straight away, if the image is not available locally it will be pulled under the hood, and stored in the Singularity cache:

```bash
singularity exec docker://ubuntu:16.04 cat /etc/os-release
```
```output
INFO:    Converting OCI blobs to SIF format
INFO:    Starting build...
Getting image source signatures
Copying blob 92473f7ef455 done
Copying blob fb52bde70123 done
Copying blob 64788f86be3f done
Copying blob 33f6d5f2e001 done
Copying config f911e561e9 done
Writing manifest to image destination
Storing signatures

[..]

INFO:    Creating SIF file...

NAME="Ubuntu"
VERSION="16.04.7 LTS (Xenial Xerus)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 16.04.7 LTS"
VERSION_ID="16.04"
HOME_URL="http://www.ubuntu.com/"
SUPPORT_URL="http://help.ubuntu.com/"
BUG_REPORT_URL="http://bugs.launchpad.net/ubuntu/"
VERSION_CODENAME=xenial
UBUNTU_CODENAME=xenial
```

By default, the cache is stored in `~/.singularity`; this location can be customised using the environment variable `SINGULARITY_CACHEDIR`. This cache directory can use a non-negligible amount of space. A subcommand, `singularity cache`, can be used to manage the cache or clear it completely.


#### Shifter

Let's download the same Ubuntu image as above, using `shifterimg`:

```bash
shifterimg pull ecpe4s/ubuntu22.04
```
```output
2022-10-11T05:25:11 Pulling Image: docker:ecpe4s/ubuntu22.04, status: READY
```

Locally stored images are managed by Shifter itself:

```bash
shifterimg images
```
```output
mycluster  docker     READY    369c047181   2022-10-11T05:25:11 ecpe4s/ubuntu22.04:latest                  
```

What's the container user with Shifter?  Let's use both `id -u` and `whoami`:

```bash
shifter --image=ecpe4s/ubuntu22.04 whoami
```
```output
tutorial
```

```bash
shifter --image=ecpe4s/ubuntu22.04 id -u
```
```output
1001
```

Again, these come from the host.

You can try more Linux commands:

```bash
shifter --image=ecpe4s/ubuntu22.04 cat /etc/os-release
```
```output
PRETTY_NAME="Ubuntu 22.04.1 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.1 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
```

NOTE: If you need to open an interactive shell in the container with Shifter, just execute `bash` in the container.


#### Podman-HPC 

### Share host environment variables

By default, host variables are shared with the container:

```bash
export HELLO="world"
```

You can access that variable both with Singularity:

```bash
singularity exec ubuntu22.04_latest.sif bash -c 'echo $HELLO'
```
```output
world
```

and with Shifter:

```bash
shifter --image=ecpe4s/ubuntu22.04 bash -c 'echo $HELLO'
```
```output
world
```

There are some additional user options worth discussing, for further tuning of the shell environment.  In some cases, *e.g.* when using Python containers, you might need to isolate the container shell from the host.  To this end, use `-e` with Singularity:

```bash
singularity exec -e ubuntu22.04_latest.sif bash -c 'echo $HELLO'
```
```output

```

and `-E` with Shifter:

```bash
shifter -E --image=ecpe4s/ubuntu22.04 bash -c 'echo $HELLO'
```
```output

```

If you need to pass variables to the container in this situation, you can use a dedicated syntax.  With Singularity it looks like:

```bash
export SINGULARITYENV_BYE="moon"
singularity exec -e ubuntu22.04_latest.sif bash -c 'echo $BYE'
```
```output
moon
```

or, from version `3.6.x` on:

```bash
singularity exec -e --env BYE="moon" ubuntu22.04_latest.sif bash -c 'echo $BYE'
```
```output
moon
```

And with Shifter:

```bash
shifter -E --env BYE="moon" --image=ecpe4s/ubuntu22.04 bash -c 'echo $BYE'
```
```output
moon
```


#### What about Podman?

By default and similar to Docker, Podman isolates host and container shell environments:

```bash
export HELLO="world"
podman run ecpe4s/ubuntu22.04 bash -c 'echo $HELLO'
```
```output

```

You can pass specific variables to the container by using the flag `-e`:

```bash
podman run -e HELLO ecpe4s/ubuntu22.04 bash -c 'echo $HELLO'
```
```output
world
```

Or even redefine variables, with the same flag:

```bash
podman run -e HELLO=moon ecpe4s/ubuntu22.04 bash -c 'echo $HELLO'
```
```output
moon
```

#### What about Podman-HPC?


### Use host directories

There is a difference in how container engines set the default working directory.  Let's see this with an example, the container image `marcodelapierre/ubuntu_workdir:18.04`, which has a custom `WORKDIR` in the Dockerfile:

```source
FROM ubuntu:18.04

WORKDIR "/workdir"
```

How does Singularity behave?

```bash
singularity exec docker://marcodelapierre/ubuntu_workdir:18.04 pwd
```
```output
/home/tutorial/sc-tutorials/exercises
```

Singularity always uses the host current working directory.  

Now, how about Shifter?

```bash
shifterimg pull marcodelapierre/ubuntu_workdir:18.04
shifter --image=marcodelapierre/ubuntu_workdir:18.04 pwd
```
```output
/home/tutorial/sc-tutorials/exercises
```

Shifter always uses the host current working directory, too.  

With both Singularity and Shifter, the container filesystem is read-only, so if you want to write output files you must do it in a bind-mounted host directory.  
Typically, HPC administrators will configure the container engine for you, so that host filesystems containing data and software are mounted by default.

In the unlikely scenario where you need to bind-mount additional paths, Singularity offers handy methods for users.  For instance:

```bash
singularity exec ubuntu22.04_latest.sif ls /data2
```
```output
ls: cannot access /data2: No such file or directory
```

```bash
singularity exec -B /data2 ubuntu22.04_latest.sif ls /data2
```
```output
file2
```

or 

```bash
export SINGULARITY_BINDPATH="/data2"
singularity exec ubuntu22.04_latest.sif ls /data2
```
```output
file2
```

Additionally singularity can use SquashFS to create a filesystem overlay that can be added as a writable filesystem in the container. This compressed filesystem can then be moved as a single file on the host filesystem and even mounted. 

First, you create the overlay of a specific size

```bash
# create the overlay
size=1000 #filesystem size in MB
filename=overlay.sqsh #filename of mountable filesystem 
singularity overlay create -s ${size} ${filename}
```

then you load this filesystem into the root of the container, allowing you to create directories in which to store data

```bash
singularity exec --overlay ${filename} ubuntu22.04_latest.sif mkdir -p /overlay/data
```

This will provide a writeable space that is isolated from the host filesystems, which can be useful if there are file quotas on the host filesystems. Files written within the container to this filesystem will not count against your quota. 

#### What about Podman?

```bash
podman run marcodelapierre/ubuntu_workdir:18.04 pwd
```
```output
/workdir
```

```bash
podman run ecpe4s/ubuntu22.04 pwd
```
```output
/
```

Similar to Docker, Podman follows `WORKDIR` as defined in the Dockerfile;  if undefined it defaults to the root dir `/`.

Also remember that, similar to Docker, Podman does not mount any host directories (although the system administrators may enable default mounting of relevant directories):

```bash
podman run -w $(pwd) marcodelapierre/ubuntu_workdir:18.04 pwd
```
```output
Error: workdir "/home/tutorial/sc-tutorials/exercises" does not exist on container d42af9a0aaed56b840b5a6be3aa0e2ba19114ead456036573491e54816e185ab
```

So, if you want to run the container in the host current directory, you need to use `-w` to specify the work directory, and `-v` to mount the desired host directory:

```bash
podman run -v $(pwd):$(pwd) -w $(pwd) marcodelapierre/ubuntu_workdir:18.04 pwd
```
```output
/home/tutorial/sc-tutorials/exercises
```

#### What about Podman-HPC?

### Do It Yourself: BLAST example

Now you're going to run a BLAST (Basic Local Alignment Search Tool) example with a container from [BioContainers](https://biocontainers.pro).  
BLAST is a tool bioinformaticians use to compare a sample genetic sequence to a database of known sequences; it's one of the most widely used bioinformatics packages.  
This example is adapted from the [BioContainers documentation](https://biocontainers-edu.readthedocs.io/en/latest/running_example.html).

For this exercise, use Singularity.  
Try and achieve what you're asked to do, use the solution only if you're lost.

Before you start, change directory to `blast`:

```bash
cd ~/sc-tutorials/exercises/blast
```

> ## Pull the image
> 
> First, download the following container image:
> 
> ```
> quay.io/biocontainers/blast:2.9.0--pl526h3066fca_4
> ```
> 
> > ## Solution
> > 
> > ```bash
> > singularity pull docker://quay.io/biocontainers/blast:2.9.0--pl526h3066fca_4
> > ```
> {: .solution}
{: .challenge}


> ## Run a test command
>
> Now, run a simple command using that image, for instance `blastp -help`, to verify that it actually works.
>
> > ## Solution
> >
> > ```bash
> > singularity exec blast_2.9.0--pl526h3066fca_4.sif blastp -help
> > ```
> >
> > ```output
> > USAGE
> >   blastp [-h] [-help] [-import_search_strategy filename]
> >
> > [..]
> >
> >  -use_sw_tback
> >    Compute locally optimal Smith-Waterman alignments?
> >
> > ```
> {: .solution}
{: .challenge}


Now, the exercise directory contains a human prion FASTA sequence, `P04156.fasta` and a gzipped reference database to blast against, `zebrafish.1.protein.faa.gz`.  
First, uncompress the database (you can use host commands for this):

```bash
gunzip zebrafish.1.protein.faa.gz
```


> ## Run the analysis
>
> We need to perform two tasks:
> 1. prepare the zebrafish database with `makeblastdb`:
>   ```bash
>   makeblastdb -in zebrafish.1.protein.faa -dbtype prot
>   ```
> 2. run the alignment with `blastp`:
>   ```bash
>   blastp -query P04156.fasta -db zebrafish.1.protein.faa -out results.txt
>   ```
>
> Start from these commands and adapt them so you can run them from within a container.
>
> > ## Solution
> >
> > ```bash
> > singularity exec blast_2.9.0--pl526h3066fca_4.sif makeblastdb -in zebrafish.1.protein.faa -dbtype prot
> > ```
> >
> > ```output
> > Building a new DB, current time: 11/16/2019 19:14:43
> > New DB name:   /home/ubuntu/singularity-containers/demos/blast_db/zebrafish.1.protein.faa
> > New DB title:  zebrafish.1.protein.faa
> > Sequence type: Protein
> > Keep Linkouts: T
> > Keep MBits: T
> > Maximum file size: 1000000000B
> > Adding sequences from FASTA; added 52951 sequences in 1.34541 seconds.
> > ```
> >
> > ```bash
> > singularity exec blast_2.9.0--pl526h3066fca_4.sif blastp -query P04156.fasta -db zebrafish.1.protein.faa -out results.txt
> > ```
> {: .solution}
{: .challenge}


The final results are stored in `results.txt`:

```bash
less results.txt
```

```output
                                                                      Score     E
Sequences producing significant alignments:                          (Bits)  Value

  XP_017207509.1 protein piccolo isoform X2 [Danio rerio]             43.9    2e-04
  XP_017207511.1 mucin-16 isoform X4 [Danio rerio]                    43.9    2e-04
  XP_021323434.1 protein piccolo isoform X5 [Danio rerio]             43.5    3e-04
  XP_017207510.1 protein piccolo isoform X3 [Danio rerio]             43.5    3e-04
  XP_021323433.1 protein piccolo isoform X1 [Danio rerio]             43.5    3e-04
  XP_009291733.1 protein piccolo isoform X1 [Danio rerio]             43.5    3e-04
  NP_001268391.1 chromodomain-helicase-DNA-binding protein 2 [Dan...  35.8    0.072
[..]
```

When you're done, quit the view by hitting the `q` button.


### Container modules with SHPC

[Singularity Registry HPC](https://singularity-hpc.readthedocs.io), or SHPC in short, is an extremely interesting project by some of the original creators of Singularity.
This utility enables the automatic deployment of so called Container Modules, using either Lmod or Environment Modules and bash functions within modulefiles.  The latter are used to wrap the container runtime syntax inside aliases that match the application executable names.  

The key advantage of SHPC is that it automates the process of downloading the container and creating the corresponding modulefile with bash function definitions.  It does so by means of a registry of recipes (currently over 300) that are ready for use.  If a recipe for a container does not exist, writing one is relatively straightforward, although out of scope for this episode.  

Let's see how we can install BLAST using SHPC.  First, let's look for available BLAST versions with `shpc show`:

```bash
shpc show --versions -f blast
```

```output
ncbi/blast:2.11.0
ncbi/blast:2.12.0
ncbi/blast:latest
ncbi/blast:2.13.0
ncbi/blast:2.14.0
ncbi/blast:2.14.1
quay.io/biocontainers/bioconductor-hicdatalymphoblast:1.30.0--r41hdfd78af_1
quay.io/biocontainers/bioconductor-hicdatalymphoblast:1.33.0--r42hdfd78af_0
quay.io/biocontainers/bioconductor-hicdatalymphoblast:1.36.0--r43hdfd78af_0
quay.io/biocontainers/blast-legacy:2.2.26--h9ee0642_3
quay.io/biocontainers/blast:2.10.1--pl526he19e7b1_3
quay.io/biocontainers/blast:2.11.0--pl5262h3289130_1
quay.io/biocontainers/blast:2.12.0--pl5262h3289130_0
...
quay.io/biocontainers/samblaster:0.1.26--h9f5acd7_2
quay.io/biocontainers/samblaster:0.1.26--h4ac6f70_4
```

And now let's install the latest BLAST biocontainer (copy-pasting the image and tag from the output above) with `shpc install`:

```bash
shpc install quay.io/biocontainers/blast:2.14.1--pl5321h6f7f691_0
```

```output
singularity pull --name /usr/local/lib/python3.10/dist-packages/singularity_hpc-0.1.26-py3.10.egg/containers/quay.io/biocontainers/blast/2.14.1--pl5321h6f7f691_0/quay.io-biocontainers-blast-2.14.1--pl5321h6f7f691_0-sha256:0fa116b90c6411d5b09cdda5ca81a857167d218c49915104e7e1588b16baedf7.sif docker://quay.io/biocontainers/blast@sha256:0fa116b90c6411d5b09cdda5ca81a857167d218c49915104e7e1588b16baedf7
INFO:    Using cached SIF image
INFO:    Using cached SIF image
Copying blob sha256:642efca944a099a40fb3c07af9503df633650ab4299bba89f04e2106bfe2d85e
Copying blob sha256:bd9ddc54bea929a22b334e73e026d4136e5b73f5cc29942896c72e4ece69b13d
Copying blob sha256:bfa1a70cade63da3b530cd6c74b1f60042ddadf4aab32369b885b55545d4156d
Copying config sha256:1db36fd7938c3efa43c779369231cee1a026b98d46cc82d2db324336bf89d8ef
Writing manifest to image destination
Storing signatures
2023/10/27 10:08:58  info unpack layer: sha256:642efca944a099a40fb3c07af9503df633650ab4299bba89f04e2106bfe2d85e
2023/10/27 10:08:58  info unpack layer: sha256:bd9ddc54bea929a22b334e73e026d4136e5b73f5cc29942896c72e4ece69b13d
2023/10/27 10:08:58  info unpack layer: sha256:bfa1a70cade63da3b530cd6c74b1f60042ddadf4aab32369b885b55545d4156d
INFO:    Creating SIF file...
Module quay.io/biocontainers/blast:2.14.1--pl5321h6f7f691_0 was created.
```

That's it!  We now have a BLAST module:

```bash
module avail quay
```

```output

-------------------------------------------- /opt/singularity-hpc/modules ---------------------------------------------
quay.io/biocontainers/blast/2.14.1--pl5321h6f7f691_0/module.tcl
```

Which we can load and use (well, the newer version does not like the *-help* flag):

```bash
module load quay.io/biocontainers/blast/2.14.1--pl5321h6f7f691_0
blastp -help
```

```output
...
DESCRIPTION
   Protein-Protein BLAST 2.14.1+
...
```

We can also see that this command is indeed bash wrapper:

```bash
more blastp
```

```bash
#!/bin/bash

script=`realpath $0`
wrapperDir=`dirname $script`/..

singularity ${SINGULARITY_OPTS} exec ${SINGULARITY_COMMAND_OPTS} -B $wrapperDir/99-shpc.sh:/.singularity.d/env/99-shpc.sh   /usr/local/lib/python3.10/dist-packages/singularity_hpc-0.1.26-py3.10.egg/containers/quay.io/biocontainers/blast/2.14.1--pl5321h6f7f691_0/quay.io-biocontainers-blast-2.14.1--pl5321h6f7f691_0-sha256:0fa116b90c6411d5b09cdda5ca81a857167d218c49915104e7e1588b16baedf7.sif /usr/local/bin/blastp "$@"
```
 
